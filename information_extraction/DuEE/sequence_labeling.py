#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# @Time:    2022-07-11 22:34
# @Author:  geng
# @Email:   yonglonggeng@163.com
# @WeChat:  superior_god
# @File:    sequence_labeling.py
# @Project: DuEE
# @Package: 
# @Ref:


"""
sequence labeling
"""
import argparse
import ast
import json
import os
import random
import warnings
from functools import partial

import numpy as np
import paddle
import paddle.nn.functional as F
from paddlenlp.data import Stack, Tuple, Pad
from paddlenlp.metrics import ChunkEvaluator
from paddlenlp.transformers import ErnieTokenizer, ErnieForTokenClassification

from utils import read_by_lines, write_by_lines, load_dict

warnings.filterwarnings('ignore')

# yapf: disable
parser = argparse.ArgumentParser(__doc__)
parser.add_argument("--num_epoch", type=int, default=1, help="è®­ç»ƒæ¬¡æ•°")
parser.add_argument("--learning_rate", type=float, default=5e-5, help="å­¦ä¹ ç‡")
parser.add_argument("--tag_path", type=str, default="conf/DuEE-Fin/trigger_tag.dict", help="æ ‡ç­¾æ–‡ä»¶è·¯å¾„")
parser.add_argument("--train_data", type=str, default="/Users/geng/Documents/data/DuEE-fin/trigger/train.tsv",
                    help="è®­ç»ƒæ•°æ®")
parser.add_argument("--dev_data", type=str, default="/Users/geng/Documents/data/DuEE-fin/trigger/dev.tsv", help="éªŒè¯æ•°æ®")
parser.add_argument("--test_data", type=str, default="/Users/geng/Documents/data/DuEE-fin/trigger/test.tsv",
                    help="æµ‹è¯•æ•°æ®")
parser.add_argument("--predict_data", type=str, default=None, help="é¢„æµ‹æ•°æ®")
parser.add_argument("--do_train", type=ast.literal_eval, default=True, help="æ˜¯å¦è®­ç»ƒ")
parser.add_argument("--do_predict", type=ast.literal_eval, default=True, help="æ˜¯å¦é¢„æµ‹")
parser.add_argument("--weight_decay", type=float, default=0.01,
                    help="æƒé‡è¡°å‡ç³»æ•°ï¼Œæ˜¯ä¸€ä¸ªfloatç±»å‹æˆ–è€…shapeä¸º[1] ï¼Œæ•°æ®ç±»å‹ä¸ºfloat32çš„Tensorç±»å‹ã€‚é»˜è®¤å€¼ä¸º0.01ã€‚")
parser.add_argument("--warmup_proportion", type=float, default=0.1, help="å¾…å®š")
parser.add_argument("--max_seq_len", type=int, default=512, help="èƒ½å¤„ç†çš„åºåˆ—æœ€å¤§é•¿åº¦")
parser.add_argument("--valid_step", type=int, default=100, help="éªŒè¯æ­¥æ•°")
parser.add_argument("--skip_step", type=int, default=20, help="è·³è·ƒæ­¥æ•°")
parser.add_argument("--batch_size", type=int, default=2, help="è®­ç»ƒæ¯ä¸ªæ‰¹æ¬¡æ‰“å¤§å°")
parser.add_argument("--checkpoints", type=str, default="checkpoints/DuEE-Fin", help="æ¨¡å‹å­˜å–")
parser.add_argument("--init_ckpt", type=str, default="outputs", help="å·²ç»è®­ç»ƒå¥½çš„æ¨¡å‹")
parser.add_argument("--predict_save_path", type=str, default="outputs", help="é¢„æµ‹ç»“æœä¿å­˜è·¯å¾„")
parser.add_argument("--seed", type=int, default=1000, help="éšæœºè¿è¡Œçš„ç§å­")
parser.add_argument('--device', choices=['cpu', 'gpu'], default="cpu",
                    help="é€‰æ‹©è¿è¡Œçš„è®¾å¤‡ï¼Œé»˜è®¤åœ¨CPUä¸Šè¿è¡Œ")
args = parser.parse_args()


# yapf: enable.


def set_seed(args):
    """sets random seed"""
    random.seed(args.seed)
    np.random.seed(args.seed)
    paddle.seed(args.seed)


@paddle.no_grad()
def evaluate(model, criterion, metric, num_label, data_loader):
    """evaluate"""
    model.eval()
    metric.reset()
    losses = []
    for input_ids, seg_ids, seq_lens, labels in data_loader:
        logits = model(input_ids, seg_ids)
        loss = paddle.mean(
            criterion(logits.reshape([-1, num_label]), labels.reshape([-1])))
        losses.append(loss.numpy())
        preds = paddle.argmax(logits, axis=-1)
        n_infer, n_label, n_correct = metric.compute(None, seq_lens, preds,
                                                     labels)
        metric.update(n_infer.numpy(), n_label.numpy(), n_correct.numpy())
        precision, recall, f1_score = metric.accumulate()
    avg_loss = np.mean(losses)
    model.train()

    return precision, recall, f1_score, avg_loss


def convert_example_to_feature(example,
                               tokenizer,
                               label_vocab=None,
                               max_seq_len=512,
                               no_entity_label="O",
                               ignore_label=-1,
                               is_test=False):
    tokens, labels = example
    tokenized_input = tokenizer(tokens,
                                return_length=True,
                                is_split_into_words=True,
                                max_seq_len=max_seq_len)

    input_ids = tokenized_input['input_ids']
    token_type_ids = tokenized_input['token_type_ids']
    seq_len = tokenized_input['seq_len']

    if is_test:
        return input_ids, token_type_ids, seq_len
    elif label_vocab is not None:
        labels = labels[:(max_seq_len - 2)]
        encoded_label = [no_entity_label] + labels + [no_entity_label]
        encoded_label = [label_vocab[x] for x in encoded_label]
        return input_ids, token_type_ids, seq_len, encoded_label


class DuEventExtraction(paddle.io.Dataset):
    """DuEventExtraction"""

    def __init__(self, data_path, tag_path):
        self.label_vocab = load_dict(tag_path)
        self.word_ids = []
        self.label_ids = []
        with open(data_path, 'r', encoding='utf-8') as fp:
            # skip the head line
            next(fp)
            for line in fp.readlines():
                words, labels = line.strip('\n').split('\t')
                words = words.split('\002')
                labels = labels.split('\002')
                self.word_ids.append(words)
                self.label_ids.append(labels)
        self.label_num = max(self.label_vocab.values()) + 1

    def __len__(self):
        return len(self.word_ids)

    def __getitem__(self, index):
        return self.word_ids[index], self.label_ids[index]


def do_train():
    # è®¾ç½®é¡¹ç›®è¿è¡Œçš„å¹³å°
    paddle.set_device(args.device)
    # TODO éœ€è¦æŸ¥è¯¢paddle.distributed.get_world_sizeè¿™ä¸ªAPIçš„ç”¨æ³•
    world_size = paddle.distributed.get_world_size()
    # TODO éœ€è¦æŸ¥è¯¢paddle.distributed.get_rankçš„ç”¨æ³•
    rank = paddle.distributed.get_rank()
    # å¦‚æœæœ‰å•è¯ï¼Œé‚£ä¹ˆè¿›è¡Œç¯å¢ƒåˆå§‹åŒ–
    if world_size > 1:
        paddle.distributed.init_parallel_env()

    # è®¾ç½®æ‰€æœ‰ç”¨åˆ°åŒ…çš„éšæœºç§å­
    set_seed(args)

    # å¯¹äºæ²¡æœ‰æ ‡ç­¾çš„ç”¨Oæ¥è¡¨ç¤º
    no_entity_label = "O"
    # éœ€è¦å¿½ç•¥çš„æ ‡ç­¾ç”¨-1è¡¨ç¤º
    ignore_label = -1

    # è·å–ernieçš„è¯è¡¨è§£æå™¨
    tokenizer = ErnieTokenizer.from_pretrained("ernie-1.0")
    # è·å–å½“å‰æ‰€æœ‰çš„æ ‡ç­¾
    label_map = load_dict(args.tag_path)
    # æ ‡ç­¾å’ŒIDç¿»è½¬
    id2label = {val: key for key, val in label_map.items()}
    # åŠ è½½åˆ†ç±»é¢„è®­ç»ƒæ¨¡å‹
    # è¿™ä¸ªæ¨¡å‹æ„é¢æ·»åŠ äº†dropout,linearå±‚ï¼Œä»¥è¾¾åˆ°ç›®æ ‡ç±»åˆ«ä¸ªæ•°
    model = ErnieForTokenClassification.from_pretrained(
        "ernie-1.0", num_classes=len(label_map))
    # è¯¦ç»†æè¿°ï¼šhttps://ew6tx9bj6e.feishu.cn/docx/doxcnB0u7eOZ8Q6WUTa2rgym94W
    model = paddle.DataParallel(model)

    print("============start train==========")
    # åŠ è½½è®­ç»ƒæ•°æ®
    train_ds = DuEventExtraction(args.train_data, args.tag_path)
    # åŠ è½½éªŒè¯æ•°æ®
    dev_ds = DuEventExtraction(args.dev_data, args.tag_path)
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_ds = DuEventExtraction(args.test_data, args.tag_path)


    # TODO è¿™ä¸ªå‡½æ•°è¿˜éœ€è¦æœ‰å¾…è¯¦ç»†æŸ¥è¯¢
    """
    åå‡½æ•°çš„ä½œç”¨ï¼šå’Œè£…é¥°å™¨ä¸€æ ·ï¼Œå®ƒå¯ä»¥æ‰©å±•å‡½æ•°çš„åŠŸèƒ½ï¼Œä½†åˆä¸å®Œæˆç­‰ä»·äºè£…é¥°å™¨ã€‚é€šå¸¸åº”ç”¨çš„åœºæ™¯æ˜¯å½“æˆ‘ä»¬è¦é¢‘ç¹è°ƒç”¨æŸä¸ªå‡½æ•°æ—¶ï¼Œå…¶ä¸­æŸäº›å‚æ•°æ˜¯å·²çŸ¥
    çš„å›ºå®šå€¼ï¼Œé€šå¸¸æˆ‘ä»¬å¯ä»¥è°ƒç”¨è¿™ä¸ªå‡½æ•°å¤šæ¬¡ï¼Œä½†è¿™æ ·çœ‹ä¸Šå»ä¼¼ä¹ä»£ç æœ‰äº›å†—ä½™ï¼Œè€Œåå‡½æ•°çš„å‡ºç°å°±æ˜¯ä¸ºäº†å¾ˆå°‘çš„è§£å†³è¿™ä¸€ä¸ªé—®é¢˜ã€‚ä¸¾ä¸€ä¸ªå¾ˆç®€å•çš„ä¾‹å­ï¼Œæ¯”
    å¦‚æˆ‘å°±æƒ³çŸ¥é“ 100 åŠ ä»»æ„æ•°çš„å’Œæ˜¯å¤šå°‘ï¼Œé€šå¸¸æˆ‘ä»¬çš„å®ç°æ–¹å¼æ˜¯è¿™æ ·çš„ï¼š
    
    # ç¬¬ä¸€ç§åšæ³•ï¼š
    def add(*args):
        return sum(args)
    
    print(add(1, 2, 3) + 100)
    print(add(5, 5, 5) + 100)
    
    # ç¬¬äºŒç§åšæ³•
    def add(*args):
        # å¯¹ä¼ å…¥çš„æ•°å€¼ç›¸åŠ åï¼Œå†åŠ ä¸Š100è¿”å›
        return sum(args) + 100
    
    print(add(1, 2, 3))  # 106
    print(add(5, 5, 5))  # 115 
    
    çœ‹ä¸Šé¢çš„ä»£ç ï¼Œè²Œä¼¼ä¹ŸæŒºç®€å•ï¼Œä¹Ÿä¸æ˜¯å¾ˆè´¹åŠ²ã€‚ä½†ä¸¤ç§åšæ³•éƒ½ä¼šå­˜åœ¨æœ‰é—®é¢˜ï¼šç¬¬ä¸€ç§ï¼Œ100è¿™ä¸ªå›ºå®šå€¼ä¼šè¿”å›å‡ºç°ï¼Œä»£ç æ€»æ„Ÿè§‰æœ‰é‡å¤ï¼›
    ç¬¬äºŒç§ï¼Œå°±æ˜¯å½“æˆ‘ä»¬æƒ³è¦ä¿®æ”¹ 100 è¿™ä¸ªå›ºå®šå€¼çš„æ—¶å€™ï¼Œæˆ‘ä»¬éœ€è¦æ”¹åŠ¨ add è¿™ä¸ªæ–¹æ³•ã€‚ä¸‹é¢æˆ‘ä»¬æ¥çœ‹ä¸‹ç”¨ parital æ€ä¹ˆå®ç°ï¼š

    from functools import partial
    
    def add(*args):
        return sum(args)
    
    add_100 = partial(add, 100)
    print(add_100(1, 2, 3))  # 106
    
    add_101 = partial(add, 101)
    print(add_101(1, 2, 3))  # 107
    """
    # æŸ¯é‡ŒåŒ–å°†æ•°æ®è½¬æ¢æˆç‰¹å¾å‘é‡
    trans_func = partial(convert_example_to_feature,
                         tokenizer=tokenizer,
                         label_vocab=train_ds.label_vocab,
                         max_seq_len=args.max_seq_len,
                         no_entity_label=no_entity_label,
                         ignore_label=ignore_label,
                         is_test=False)
    # lambdå‡½æ•°
    """
    å°†lambdaå‡½æ•°èµ‹å€¼ç»™ä¸€ä¸ªå˜é‡ï¼Œé€šè¿‡è¿™ä¸ªå˜é‡é—´æ¥è°ƒç”¨è¯¥lambdaå‡½æ•°ã€‚
    def sum(x,y):
        return x+y
    print(sum(1,2))
    
    ä½¿ç”¨lambdaå‡½æ•°
    sum = lambda x,y : x+y
    print(sum(1,2))
    """

    """
    mapå‡½æ•°ï¼šmap(function, iterable, ...)
    >>> def square(x) :         # è®¡ç®—å¹³æ–¹æ•°
    ...     return x ** 2
    ... 
    >>> map(square, [1,2,3,4,5])    # è®¡ç®—åˆ—è¡¨å„ä¸ªå…ƒç´ çš„å¹³æ–¹
    <map object at 0x100d3d550>     # è¿”å›è¿­ä»£å™¨
    >>> list(map(square, [1,2,3,4,5]))   # ä½¿ç”¨ list() è½¬æ¢ä¸ºåˆ—è¡¨
    [1, 4, 9, 16, 25]
    >>> list(map(lambda x: x ** 2, [1, 2, 3, 4, 5]))   # ä½¿ç”¨ lambda åŒ¿åå‡½æ•°
    [1, 4, 9, 16, 25]
    >>> 
    
    
    """
    batchify_fn = lambda samples, fn=Tuple(  # è¿™é‡Œæ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œ
        Pad(axis=0, pad_val=tokenizer.vocab[tokenizer.pad_token], dtype='int32'
            ),  # è¾“å…¥æ–‡å­—çš„id
        Pad(axis=0, pad_val=tokenizer.vocab[tokenizer.pad_token], dtype='int32'
            ),  # è¾“å…¥æ–‡å­—ç±»å‹çš„id
        Stack(dtype='int64'),  # åºåˆ—çš„é•¿åº¦
        Pad(axis=0, pad_val=ignore_label, dtype='int64')  # æ ‡ç­¾
    ): fn(list(map(trans_func, samples)))  # æ•°æ®è½¬å¼ é‡å‡½æ•° å’Œ æ ·æœ¬æ•°æ®
    # ã€æ³¨æ„ğŸ“¢ã€‘å†’å·å‰éƒ½æ˜¯å‚æ•°å“ˆï¼Œå†’å·åæ˜¯å‡½æ•°é€»è¾‘

    # è¿”å›æ ·æœ¬ä¸‹æ ‡æ•°ç»„çš„è¿­ä»£å™¨
    batch_sampler = paddle.io.DistributedBatchSampler(
        train_ds, batch_size=args.batch_size, shuffle=True)

    # DataLoaderï¼Œè¿­ä»£ dataset æ•°æ®çš„è¿­ä»£å™¨ï¼Œè¿­ä»£å™¨è¿”å›çš„æ•°æ®ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªTensorã€‚
    # è¿™é‡Œbatch_samplerå·²ç»æ˜¯æ•°å­—äº†ï¼Œ
    train_loader = paddle.io.DataLoader(dataset=train_ds,
                                        batch_sampler=batch_sampler,
                                        collate_fn=batchify_fn)

    dev_loader = paddle.io.DataLoader(dataset=dev_ds,
                                      batch_size=args.batch_size,
                                      collate_fn=batchify_fn)

    # ç›®æµ‹è¿™é‡Œçš„æµ‹è¯•æ•°æ®é›†æ²¡æœ‰ç”¨åˆ°
    test_loader = paddle.io.DataLoader(dataset=test_ds,
                                       batch_size=args.batch_size,
                                       collate_fn=batchify_fn)

    # è®¡ç®—è®­ç»ƒçš„æ­¥æ•°
    # å…¬å¼ï¼šè®­ç»ƒæ­¥æ•°=è®­ç»ƒæ¥é•¿åº¦*è¿­ä»£æ¬¡æ•°
    num_training_steps = len(train_loader) * args.num_epoch

    # ç”Ÿæˆå‚æ•°ï¼Œéå†æ¨¡å‹æµ‹å‚æ•°åç§°ï¼Œæ’é™¤æ‰å¸¦åç½®ï¼Œæ­£åˆ™çš„å‚æ•°
    decay_params = [
        p.name for n, p in model.named_parameters()
        if not any(nd in n for nd in ["bias", "norm"])
    ]

    # ä¼˜åŒ–å™¨éƒ¨åˆ†ï¼Œä½¿ç”¨AdamW
    # AdamWä¼˜åŒ–å™¨å‡ºè‡ª DECOUPLED WEIGHT DECAY REGULARIZATIONï¼Œç”¨æ¥è§£å†³ Adam ä¼˜åŒ–å™¨ä¸­L2æ­£åˆ™åŒ–å¤±æ•ˆçš„é—®é¢˜ã€‚
    optimizer = paddle.optimizer.AdamW(
        learning_rate=args.learning_rate,
        parameters=model.parameters(),
        weight_decay=args.weight_decay,  # æƒé‡è¡°å‡ç³»æ•°ï¼Œæ˜¯ä¸€ä¸ªfloatç±»å‹æˆ–è€…shapeä¸º[1] ï¼Œæ•°æ®ç±»å‹ä¸ºfloat32çš„Tensorç±»å‹ã€‚é»˜è®¤å€¼ä¸º0.01ã€‚
        apply_decay_param_fun=lambda x: x in decay_params)  # ä¼ å…¥å‡½æ•°æ—¶ï¼Œåªæœ‰å¯ä»¥ä½¿ apply_decay_param_fun(Tensor.name)==True
    # çš„Tensorä¼šè¿›è¡Œweight decayæ›´æ–°ã€‚åªæœ‰åœ¨æƒ³è¦æŒ‡å®šç‰¹å®šéœ€è¦è¿›è¡Œ
    # weight decayæ›´æ–°çš„å‚æ•°æ—¶ä½¿ç”¨ã€‚é»˜è®¤å€¼ä¸ºNoneã€‚
    """
    from paddlenlp.metrics import ChunkEvaluator

    num_infer_chunks = 10
    num_label_chunks = 9
    num_correct_chunks = 8

    label_list = [1,1,0,0,1,0,1]
    evaluator = ChunkEvaluator(label_list)
    evaluator.update(num_infer_chunks, num_label_chunks, num_correct_chunks)
    precision, recall, f1 = evaluator.accumulate()
    print(precision, recall, f1)
    # 0.8 0.8888888888888888 0.8421052631578948
    """
    # è¿™é‡Œè¯»å–æ˜¯information_extraction/DuEE/conf/DuEE-Fin/trigger_tag.dictè¿™ä¸ªå­—å…¸ï¼Œkeysæ˜¯æ•°å­—
    metric = ChunkEvaluator(label_list=train_ds.label_vocab.keys(),
                            suffix=False)

    # å®šä¹‰æŸå¤±å‡½æ•°
    # è¯¥OPè®¡ç®—è¾“å…¥inputå’Œæ ‡ç­¾labelé—´çš„äº¤å‰ç†µæŸå¤± ï¼Œå®ƒç»“åˆäº† LogSoftmax å’Œ NLLLoss çš„OPè®¡ç®—ï¼Œå¯ç”¨äºè®­ç»ƒä¸€ä¸ª n ç±»åˆ†ç±»å™¨ã€‚
    # from paddle.nn import CrossEntropyLoss
    criterion = paddle.nn.loss.CrossEntropyLoss(ignore_index=ignore_label)

    # åˆå§‹åŒ–è¿­ä»£æ­¥æ•°å’Œf1-score
    step, best_f1 = 0, 0.0
    # æ¨¡å‹è®­ç»ƒï¼Œè¿™é‡Œæ˜¯ä¹‹å‰çš„é‚£ä¸ªlayerå±‚
    model.train()

    # æ ¹æ®è¿­ä»£æ¬¡æ•°å¾ªç¯
    for epoch in range(args.num_epoch):
        # ä»¥dataloaderä¸€ä¸ªè¿›è¡Œéå†
        # åŒ…æ‹¬ï¼šè¾“å…¥æ–‡å­—çš„id,è¾“å…¥æ–‡å­—çš„ç±»å‹id,åºåˆ—çš„é•¿åº¦ï¼Œä»¥åŠæ ‡ç­¾
        tag=0 # tagæµ‹è¯•ç”¨
        for idx, (input_ids, token_type_ids, seq_lens,
                  labels) in enumerate(train_loader):

            tag+=1
            if tag%50==0:print("ç¬¬",epoch,"é","æ ‡å¿—ä½ï¼š",tag)
            if tag>200:break

            # æ ¹æ®æ¨¡å‹è®¡ç®—è¾“å‡ºå±‚çš„ç»“æœï¼Œæ˜¯ä¸€ä¸ªæœ‰ç±»åˆ«ä¸ªæ•°ä¸ªlinear,éœ€è¦æŠŠæœ€åä¸€å±‚è½¬æ¢æˆæ ‡ç­¾ä¸ªæ•°ä¸ªï¼Œå…¶å®è¿™å—æœ¬æ¥è¾“å‡ºä¹Ÿæ˜¯æ ‡ç­¾ä¸ªï¼Œä½†æ˜¯æ˜¯æœ‰æ‰¹æ¬¡çš„
            logits = model(input_ids,
                           token_type_ids).reshape([-1, train_ds.label_num])
            # ä½¿ç”¨äº¤å‰ç†µè®¡ç®—æ ‡ç­¾çš„å€¼å’ŒçœŸå®å€¼ä¹‹é—´çš„å·®è·
            loss = paddle.mean(criterion(logits, labels.reshape([-1])))
            # åˆ©ç”¨æ¡†æ¶åå‘æ±‚å¯¼
            loss.backward()
            # ä½¿ç”¨ä¼˜åŒ–å™¨
            optimizer.step()
            # æ¸…é™¤ä¹‹å‰çš„æ¢¯åº¦
            optimizer.clear_grad()

            # itemè¿”å›æŒ‡å®šç´¢å¼•å¤„çš„å€¼
            loss_item = loss.numpy().item()
            # è·³è·ƒå¼æ‰“å°æ—¥å¿—ï¼Œæ‰“å°è®­ç»ƒçš„
            if step > 0 and step % args.skip_step == 0 and rank == 0:
                print(
                    f'train epoch: {epoch} - step: {step} (total: {num_training_steps}) - loss: {loss_item:.6f}'
                )

            # è·³è·ƒå¼æ‰“å°éªŒè¯éƒ¨åˆ†
            if step > 0 and step % args.valid_step == 0 and rank == 0:
                # åˆ°ç‰¹å®šæ­¥æ•°ï¼Œå¼€å§‹è°ƒç”¨éªŒè¯æ•°æ®
                p, r, f1, avg_loss = evaluate(model, criterion, metric,
                                              len(label_map), dev_loader)
                print(f'dev step: {step} - loss: {avg_loss:.5f}, precision: {p:.5f}, recall: {r:.5f}, ' \
                      f'f1: {f1:.5f} current best {best_f1:.5f}')
                # åªå­˜å–è¾ƒå¥½è®°å½•
                if f1 > best_f1:
                    # æ›´æ–°è®°å½•
                    best_f1 = f1
                    print(f'==============================================save best model ' \
                          f'best performerence {best_f1:5f}')
                    # ä¿å­˜æ¨¡å‹
                    paddle.save(model.state_dict(),
                                '{}/best.pdparams'.format(args.checkpoints))
            # ç¬¬äºŒä¸ªè½®æ¬¡çš„è¿­ä»£
            step += 1

    # æœ€åä¸€ä¸ªæ¨¡å‹çš„ä¿å­˜ï¼Œæ— è®ºæ¨¡å‹å¥½å
    if rank == 0:
        paddle.save(model.state_dict(),
                    '{}/final.pdparams'.format(args.checkpoints))


def do_predict():
    # è®¾ç½®ç¨‹åºè¿è¡Œè®¾å¤‡
    paddle.set_device(args.device)

    # è¯è¡¨tokenizer
    tokenizer = ErnieTokenizer.from_pretrained("ernie-1.0")
    # åŠ è½½æ ‡ç­¾è¯å…¸
    label_map = load_dict(args.tag_path)
    # ç¿»è½¬æ ‡ç­¾è¯å…¸
    id2label = {val: key for key, val in label_map.items()}
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    model = ErnieForTokenClassification.from_pretrained(
        "ernie-1.0", num_classes=len(label_map))

    # æ²¡æœ‰å®ä½“æ ‡ç­¾çš„ç”¨Oè¡¨ç¤º
    no_entity_label = "O"
    # å°±æ•£æ ‡ç­¾çš„é•¿åº¦
    ignore_label = len(label_map)

    print("============start predict==========")
    # åˆ¤æ–­æ˜¯å¦æœ‰åˆå§‹åŒ–æ–‡ä»¶å¤¹ï¼ŒåŠåˆ¤æ–­æ˜¯å¦æ˜¯æ–‡ä»¶
    if not args.init_ckpt or not os.path.isfile(args.init_ckpt):
        raise Exception("init checkpoints {} not exist".format(args.init_ckpt))
    else:
        # åŠ è½½é™æ€æ¨¡å‹
        state_dict = paddle.load(args.init_ckpt)
        # æ¨¡å‹å¡«å……å‚æ•°
        model.set_dict(state_dict)
        print("Loaded parameters from %s" % args.init_ckpt)

    # load data from predict file
    # æŒ‰è¡Œè¯»å–éœ€è¦é¢„æµ‹çš„æ•°æ®
    sentences = read_by_lines(args.predict_data)  # origin data format
    # å¥å­ç­ä¸€è¡Œæ˜¯ä¸€ä¸ªjson,è½¬æ¢æˆé•¿åˆ—è¡¨å†…åµŒjson
    sentences = [json.loads(sent) for sent in sentences]

    # ç¼–ç è¾“å…¥çš„åˆ—è¡¨
    encoded_inputs_list = []
    # ä¾¿åˆ©æ¯ä¸€å¥
    for sent in sentences:
        # æ›¿æ¢æ‰æ¯ä¸€å¥ç©ºæ ¼
        sent = sent["text"].replace(" ", "\002")
        # è½¬æ¢å¥å­ä¸ºå¯¹åº”çš„æ–‡å­—id,ç±»å‹id,å’Œåºåˆ—é•¿åº¦
        input_ids, token_type_ids, seq_len = convert_example_to_feature(
            [list(sent), []],
            tokenizer,
            max_seq_len=args.max_seq_len,
            is_test=True)
        # æ‹¼æ¥åˆ°é¢„æµ‹æ•°æ®é›†åˆ—è¡¨ä¸Š
        encoded_inputs_list.append((input_ids, token_type_ids, seq_len))

    # æ„å»ºä¸€ä¸ªæ•°æ®é›†
    batchify_fn = lambda samples, fn=Tuple(
        Pad(axis=0, pad_val=tokenizer.vocab[tokenizer.pad_token], dtype='int32'
            ),  # input_ids
        Pad(axis=0, pad_val=tokenizer.vocab[tokenizer.pad_token], dtype='int32'
            ),  # token_type_ids
        Stack(dtype='int64')  # sequence lens
    ): fn(samples)
    # æŒ‰ç…§æ‰¹æ¬¡å¤§å°è¿›è¡Œåˆ†å‰²
    batch_encoded_inputs = [
        encoded_inputs_list[i:i + args.batch_size]
        for i in range(0, len(encoded_inputs_list), args.batch_size)
    ]
    results = []
    # æ¨¡å‹éªŒè¯
    model.eval()
    # éå†æ¯ä¸€æ‰¹çš„æ•°æ®
    for batch in batch_encoded_inputs:
        # è·å–ä¸€ä¸ªæ‰¹æ¬¡æ•°æ®
        input_ids, token_type_ids, seq_lens = (batch)
        # å¯¹token idè½¬å¼ é‡
        input_ids = paddle.to_tensor(input_ids)
        # å¯¹token type IDè½¬å¼ é‡
        token_type_ids = paddle.to_tensor(token_type_ids)
        # æ¨¡å‹è¿›è¡Œè®¡ç®—
        logits = model(input_ids, token_type_ids)
        # å¯¹linearå±‚ç»“æœè¿›è¡Œsoftmaxå½’ä¸€åŒ–ï¼Œæ–¹ä¾¿è®¡ç®—æ¯ä¸€ä¸ªç±»åˆ«çš„æ¦‚ç‡ï¼Œæ˜¯åœ¨æœ€åä¸€ä¸ªè½´ä¸Šçš„ï¼Œç¬¬ä¸€ä¸ªè½´é»˜è®¤æ˜¯æ‰¹æ¬¡
        probs = F.softmax(logits, axis=-1)
        # è·å–æœ€åä¸€ç»´å°šï¼Œæ•°æ®æœ€å¤§çš„ä¸‹æ ‡
        probs_ids = paddle.argmax(probs, -1).numpy()
        # å°†æ¦‚ç‡è½¬æ¢æˆæ¦‚ç‡
        probs = probs.numpy()
        # æŒ‰ç…§è®¡ç®—çš„æ¦‚ç‡å’Œä»¥åŠï¼Œæ¯ä¸€ä¸ªä½ç½®ç±»å‹idï¼Œå’Œåºåˆ—é•¿åº¦ï¼Œéå†
        for p_list, p_ids, seq_len in zip(probs.tolist(), probs_ids.tolist(),
                                          seq_lens.tolist()):
            prob_one = [
                p_list[index][pid]  # æ¯ä¸ªå­—ç¬¦å¯¹åº”çš„ç±»åˆ«
                for index, pid in enumerate(p_ids[1:seq_len - 1])
            ]
            # æ ¹æ®æœ‰æ•ˆåºåˆ—ï¼Œåˆ‡ç‰‡æœ‰æ•ˆå­—ç¬¦ï¼Œå¹¶æŸ¥è¯¢å¯¹åº”çš„æ ‡ç­¾
            label_one = [id2label[pid] for pid in p_ids[1:seq_len - 1]]
            # æŠŠç»“æœï¼Œä»¥åŠå¯¹åº”çš„æ ‡ç­¾ç»„åˆèµ·æ¥
            results.append({"probs": prob_one, "labels": label_one})
    # æ–­è¨€ å¥å­é•¿åº¦å’Œç»“æœé•¿åº¦æ˜¯å¦ä¸€è‡´
    assert len(results) == len(sentences)
    # éå†å¯¹åº”å¥å­ä»¥åŠé¢„æµ‹ç»“æœ
    for sent, ret in zip(sentences, results):
        sent["pred"] = ret

    # å°†ç»“æœè½¬æ¢å±‚jsonç±»å‹ï¼Œå¹¶æ‹¼æ¥æˆåˆ—è¡¨
    sentences = [json.dumps(sent, ensure_ascii=False) for sent in sentences]
    # æŒ‰è¡Œå†™åˆ°æœ¬åœ°
    write_by_lines(args.predict_save_path, sentences)
    # æ‰“å°å·²å¤„ç†å¥å­é•¿åº¦ï¼Œä»¥åŠç»“æœä¿å­˜çš„è·¯å¾„
    print("save data {} to {}".format(len(sentences), args.predict_save_path))


if __name__ == '__main__':
    # åˆ¤æ–­do_trainå‚æ•°æ˜¯å¦ä¸ºTrue
    if args.do_train:
        print("train")
        do_train()
    elif args.do_predict:
        print("predict")
        do_predict()
